% Christian Hubbs
% 28.08.2019

% because I always forget, make sure to run the bibtex command on THIS document!

\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage[pdftex]{graphicx}
\graphicspath{{./images/}}
\usepackage{setspace}
\usepackage{color}
\usepackage{float}
%\hypersetup{draft}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}
\usepackage{subcaption}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\usepackage{chngcntr} % Adjusts the equation numbering by section
%\counterwithin*{equation}{section}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % Provides norms in mathmode
\newcommand{\citetemp}[1]{(#1)}
\usepackage{wrapfig}
\usepackage{hanging}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage[toc,page]{appendix}
\usepackage{rotating}
\usepackage{multirow}
\newcommand{\textcite}[1]{\citeauthor{#1}, \citeyear{#1}}
\providecommand{\keywords}[1]{\textit{Keywords:} #1}
%\usepackage{biblatex}
\doublespacing
 
%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}

\title{OR-Gym: A Reinforcement Learning Library for Operations Research Problems}
\author{
	Christian D.~Hubbs,\thanks{Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 15123} \\
	Hector Parra Perez,\footnotemark[1] \\
	Owais Sarwar,\footnotemark[1]\\
	Nikolaos V. Sahinidis,\footnotemark[1] \\
	Ignacio E. Grossmann,\footnotemark[1] \\
	John M. Wassick\thanks{Dow Chemical, Digital Fulfillment Center, Midland, MI 48667}
}

\begin{document}

\maketitle

\begin{abstract}
We introduce OR-Gym, an open-source benchmark in the form of OpenAI Gym for developing reinforcement learning algorithms to address operations research problems.
Reinforcement learning has been widely applied to game-playing and surpassed the best human-level performance in many domains, yet there are few use-cases in industrial or commercial settings.
We apply reinforcement learning to the knapsack, bin packing, travelling salesman, news vendor, max pooling, portfolio optimization, and vehicle routing problems as well as more general, multi-period resource task networks. 
These problems cover logistics, finance, engineering, and are common in many business operation settings.
In each case, we select a prototypical version from the literature to benchmark reinforcement learning and other optimal approaches against. 
\end{abstract}

\keywords{Machine Learning, Reinforcement Learning, Optimization, Scheduling, Stochastic Programming}

\section{Introduction}

Reinforcement learning (RL) is a branch of machine learning that seeks to make a series of sequential decisions to maximize a reward \citep{Sutton2018}.
The technique has recieved widespread attention in game-playing, whereby RL approaches have beaten some of the world's best human players in Go, DOTA2, and StarCraft II to name a few (\citet{Silver2017}, \citet{Berner2019a}, \citet{Vinyals2019}). 
There is a growing body of literature that is applying RL techniques to existing OR problems.
% Basically took these next few lines from Balaji 2019 for a placeholder 
\citet{Bello2019} applies RL to the travelling salesman problem (TSP) and knapsack problems (KP). 
\citet{Kool2019} apply RL to the TSP and variants such as the vehicle routing problem (VRP) and a prize-collection variant. 
\citet{Nazari2018} solve static and online versions of the VRP.
\citet{Kong2019} use RL to solve an online knapsack problem, secretary and adwords problems.
\citet{Oroojlooyjadid2017} apply RL to the beer game.
\citet{Lin2018} us RL for taxi fleet management. 
% End plagarism
\citet{Balaji2019} provided versions of online bin packing, news vendor, and vehicle routing problems as well as models for RL benchmarks.

Despite these successes only a handful of industrial examples exist in the literature. 
\citet{Hubbs2020} use RL to schedule a single-stage chemical reactor under uncertain demand which outperforms naive optimization models. 
% Add other industrial applications as they may arise. There are some other refs in my paper.

We seek to provide a standardized library for the research community who wishes to extend RL into commercial applications by building on top of the preceeding work and releasing OR-Gym, a single library that relies on the familiar OpenAI interface for RL \citep{Brockman2016}, but contains problems relevant for industrial use.
To this end, we have incorporated the benchmarks in \citet{Balaji2019}, while extending the library to address the TSP, KP, portfolio optimization, and multi-period resource task networks (RTN). 
RL problems are formulated as Markov Decision Processes (MDP), meaning they are sequential decision making problems, often times probabalistic in nature, and rely on the current state of the system to capture all relevant information for determining future states. % Can clean up this language a bit.
This framework is not widely used in the optimization community, so we make explicit our thought process as we reformulate many optimization problems to fit into the MDP mold without loss of generality.
It is our goal that this work encourages further development and integration of RL into optimization and the OR community while also opening the RL community to many of the problems and challenges that the OR community has been wrestling with for decades.

\section{Knapsack}

The Knapsack Problem (KP) was first introduced by \citet{Mathews1896}. 
It is a NP-hard combinatorial optimization problem that seeks to maximize the value of items ($v_i$) with a given weight ($w_i$)contained in a knapsack subject to a weight limit $W$. 

There are a few versions of the problem in the literature, the undbounded KP, bounded, (anything with stochastic elements?)

\textcolor{red}{Also add info about practical applications, additional citations, etc. to put this particular problem into more context. Is there a motivating example we can draw on?}

\subsection{Unbounded Knapsack}

The unbounded version of the knapsack problem (UPK) can be formulated as an optimization problem as follows:

\begin{equation}
\textrm{max} \; z = \sum_i v_i x_i
\end{equation}

\begin{equation}
\textrm{s.t.} \; \sum_i w_i x_i \leq W
\end{equation}

\begin{equation}
x_i \in [0, 1]
\end{equation}

\begin{equation}
v_i, w_i \in \rm I\!R^+
\end{equation}

where $x_i$ denotes a binary decision variable to include or exclude an item from the knapsack. 
This can be solved as an integer programming problem using algorithms such as branch-and-bound to maximize the objective function. 
In the unbounded case, the number of times the algorithm can select a given item is unlimited.

\subsection{Bounded Knapsack}

The bounded knapsack problem (BKP) differs from the unbounded case in that each item can only be selected a limited number of times. 
This introduces an additional constraint into our model.

\begin{equation}
\textrm{max} \; z = \sum_i v_i x_i
\end{equation}

\begin{equation}
\textrm{s.t.} \; \sum_i w_i x_i \leq W
\end{equation}

\begin{equation}
\sum_i x_i \leq N_i
\end{equation}

\begin{equation}
x_i \in [0, 1]
\end{equation}

\begin{equation}
v_i, w_i \in \rm I\!R^+
\end{equation}

where $N_i$ is the number of times an item can be selected.

\subsection{Stochastic Bounded Knapsack}
SBKP?

\subsection{Benchmark}

The UKP and BKP are solved as integer programs using Gurobi 8.2 and Pyomo 5.6.2 to optimality.
This method will yield the optimal solution for comparison of the RL model. 
Additionally, we will employ a heuristic solution that uses the value to weight ratio of each item to fill the knapsack first with the items that have the highest value to weight ratio before moving to the second item in the list and so forth until the weight limit is reached.
Each of these methods is available within the OR-Gym library.
The UKP is given by Knapsack-v0, BKP by Knapsack-v1. 
Example code is given to enable reproducibility.

\subsection{Problem Formulation}

While IP solutions seek to solve the problem simultaneously, RL requires a MDP formulation which relies on sequential decision making.
In this case, the RL model must select successive items to place in the knapsack until the limit is reached.
This approach seems more akin to how a human would pack a bag, placing one in at a time until the knapsack is full.
The human knapsack-packer can always remove an item if she determines it does not fit or finds a better item, our RL system, however cannot.
Once an item is selected for inclusion, it must remain. 

The RL system requires a state to make each decision.
In each case, the state could be as simple as the current weight load inside the knapsack, at which point the system would make random selections until it learned a suitable sequence of actions to fill the knapsack.
This approach certainly does not bode well for transfer to new sets of items or knapsacks with different weight limits.
Moreover, the single-state input is wasting potentially valuable information such as the value of the items, the weight, the ultimate carrying capacity of the knapsack, and in the bounded cases, the number of items available to be selected.
For this reason, we provide each of these values as part of the state output.
In the UKP case, the environment returns a tuple of three vectors: item value, item weight, and a vector containing the current load in the knapsack and weight limit.
For the BKP case, we add a fourth vector containing the number of items available to be selected.

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Bin Packing}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{News Vendor}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Travelling Salesman}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Vehicle Routing}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Max Pooling}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Portfolio Optimization}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Resource Task Network}

\subsection{Problem Formulation}

\subsection{Benchmark}

\subsection{Reinforcement Learning Algorithm}

\subsection{Results}

\section{Conclusion}

\subsection{Future Work}

\newpage

\bibliography{references.bib}

\end{document}