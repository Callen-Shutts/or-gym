{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import time\n",
    "import ray\n",
    "from ray import tune\n",
    "from or_gym.algos import rl_utils\n",
    "# %matplotlib inline\n",
    "# plt.style.use('cmu_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 15:27:36,445\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-21 15:27:36,451\tINFO resource_spec.py:216 -- Starting Ray with 4.35 GiB memory available for workers and up to 2.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\tKnapsack-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 15:27:37,063\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_675408dc: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/4.35 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (1 RUNNING, 2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th><th>lr  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_675408dc</td><td>RUNNING </td><td>     </td><td>    </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_67546818</td><td>PENDING </td><td>     </td><td>    </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_6754f238</td><td>PENDING </td><td>     </td><td>    </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 15:27:37,209\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_67546818: Setting up new remote runner.\n",
      "2020-03-21 15:27:37,320\tINFO ray_trial_executor.py:121 -- Trial PPO_Knapsack-v0_6754f238: Setting up new remote runner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:27:40,753\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:27:40,756\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:27:40,970\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:27:40,973\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:27:40,972\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:27:40,975\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:27:44,802\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:27:44,996\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:27:45,217\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_Knapsack-v0_675408dc:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 20.683937823834196\n",
      "  episode_reward_max: 402.0\n",
      "  episode_reward_mean: 256.0362694300518\n",
      "  episode_reward_min: 140.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 193\n",
      "  experiment_id: 3f0d15625fc04812b1ee5b0a15e27735\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9107.232\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 1.6574641466140747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 13.0006742477417\n",
      "        policy_loss: 0.6163644194602966\n",
      "        total_loss: 17563.38671875\n",
      "        vf_explained_var: -1.9227304193236705e-09\n",
      "        vf_loss: 17560.171875\n",
      "    load_time_ms: 117.292\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 7274.611\n",
      "    update_time_ms: 565.657\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.44800000000001\n",
      "    ram_util_percent: 54.952\n",
      "  pid: 20699\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1103194526838261\n",
      "    mean_inference_ms: 1.046883943705998\n",
      "    mean_processing_ms: 0.2137575409347431\n",
      "  time_since_restore: 17.154648065567017\n",
      "  time_this_iter_s: 17.154648065567017\n",
      "  time_total_s: 17.154648065567017\n",
      "  timestamp: 1584822482\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 675408dc\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6/8 CPUs, 0/1 GPUs, 0.0/4.35 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_675408dc</td><td>RUNNING </td><td>192.168.0.11:20699</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.1546</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 256.036</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_67546818</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_6754f238</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">        </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:28:02,117\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (17560.171875) compared to the policy loss (0.6163644194602966). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_67546818:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 20.683937823834196\n",
      "  episode_reward_max: 420.0\n",
      "  episode_reward_mean: 252.6321243523316\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 193\n",
      "  experiment_id: c0b1e1281d2d4a4385e0b44b4c6a1487\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9316.703\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.263301372528076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.035863086581230164\n",
      "        policy_loss: -0.04704999178647995\n",
      "        total_loss: 17075.76953125\n",
      "        vf_explained_var: 2.2413269107346423e-05\n",
      "        vf_loss: 17075.80859375\n",
      "    load_time_ms: 135.613\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 7230.302\n",
      "    update_time_ms: 564.116\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.43199999999999\n",
      "    ram_util_percent: 55.04\n",
      "  pid: 20695\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10449813026632257\n",
      "    mean_inference_ms: 1.0111535736633164\n",
      "    mean_processing_ms: 0.2043521335738148\n",
      "  time_since_restore: 17.314058303833008\n",
      "  time_this_iter_s: 17.314058303833008\n",
      "  time_total_s: 17.314058303833008\n",
      "  timestamp: 1584822482\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '67546818'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:28:02,505\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (17075.80859375) compared to the policy loss (-0.04704999178647995). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_6754f238:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 20.858638743455497\n",
      "  episode_reward_max: 417.0\n",
      "  episode_reward_mean: 256.64921465968587\n",
      "  episode_reward_min: 145.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 191\n",
      "  experiment_id: 4d8aa7eb24f74eba9f13241a555b02f6\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9307.047\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.29777193069458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005296625895425677\n",
      "        policy_loss: -0.007658547721803188\n",
      "        total_loss: 20047.580078125\n",
      "        vf_explained_var: 0.00018077318964060396\n",
      "        vf_loss: 20047.5859375\n",
      "    load_time_ms: 87.195\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 7185.769\n",
      "    update_time_ms: 533.087\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.36\n",
      "    ram_util_percent: 55.083999999999996\n",
      "  pid: 20702\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10629654884100014\n",
      "    mean_inference_ms: 1.0123761765094612\n",
      "    mean_processing_ms: 0.20721023662541396\n",
      "  time_since_restore: 17.16783356666565\n",
      "  time_this_iter_s: 17.16783356666565\n",
      "  time_total_s: 17.16783356666565\n",
      "  timestamp: 1584822482\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6754f238\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:28:02,556\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20047.5859375) compared to the policy loss (-0.007658547721803188). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_675408dc:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 33.2\n",
      "  episode_reward_max: 704.0\n",
      "  episode_reward_mean: 449.60833333333335\n",
      "  episode_reward_min: 269.0\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 313\n",
      "  experiment_id: 3f0d15625fc04812b1ee5b0a15e27735\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8861.982\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 2.250545024871826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.5735264420509338\n",
      "        policy_loss: 0.08069393038749695\n",
      "        total_loss: 42094.94140625\n",
      "        vf_explained_var: -2.6918225870531387e-08\n",
      "        vf_loss: 42094.6796875\n",
      "    load_time_ms: 66.523\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 6004.803\n",
      "    update_time_ms: 287.37\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.9578947368421\n",
      "    ram_util_percent: 56.768421052631574\n",
      "  pid: 20699\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10524158432489333\n",
      "    mean_inference_ms: 0.9622443632667713\n",
      "    mean_processing_ms: 0.19983422263028275\n",
      "  time_since_restore: 30.54728865623474\n",
      "  time_this_iter_s: 13.392640590667725\n",
      "  time_total_s: 30.54728865623474\n",
      "  timestamp: 1584822495\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 675408dc\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6/8 CPUs, 0/1 GPUs, 0.0/4.35 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_675408dc</td><td>RUNNING </td><td>192.168.0.11:20699</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.5473</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 449.608</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_67546818</td><td>RUNNING </td><td>192.168.0.11:20695</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.3141</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 252.632</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_6754f238</td><td>RUNNING </td><td>192.168.0.11:20702</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.1678</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\"> 256.649</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:28:15,530\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (42094.6796875) compared to the policy loss (0.08069393038749695). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_67546818:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 21.3031914893617\n",
      "  episode_reward_max: 404.0\n",
      "  episode_reward_mean: 271.4574468085106\n",
      "  episode_reward_min: 142.0\n",
      "  episodes_this_iter: 188\n",
      "  episodes_total: 381\n",
      "  experiment_id: c0b1e1281d2d4a4385e0b44b4c6a1487\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9051.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.2162041664123535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03205427899956703\n",
      "        policy_loss: -0.04727276414632797\n",
      "        total_loss: 17675.236328125\n",
      "        vf_explained_var: 8.031244760786649e-06\n",
      "        vf_loss: 17675.275390625\n",
      "    load_time_ms: 78.173\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 5939.079\n",
      "    update_time_ms: 283.695\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.49499999999999\n",
      "    ram_util_percent: 56.75\n",
      "  pid: 20695\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.09971805668342057\n",
      "    mean_inference_ms: 0.9341342138388864\n",
      "    mean_processing_ms: 0.19693916968264705\n",
      "  time_since_restore: 30.78986620903015\n",
      "  time_this_iter_s: 13.475807905197144\n",
      "  time_total_s: 30.78986620903015\n",
      "  timestamp: 1584822495\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '67546818'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:28:15,996\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (17675.275390625) compared to the policy loss (-0.04727276414632797). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_6754f238:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 20.869791666666668\n",
      "  episode_reward_max: 407.0\n",
      "  episode_reward_mean: 259.546875\n",
      "  episode_reward_min: 150.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 383\n",
      "  experiment_id: 4d8aa7eb24f74eba9f13241a555b02f6\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 9062.064\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.29658317565918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.000857196282595396\n",
      "        policy_loss: -0.008683229796588421\n",
      "        total_loss: 21030.609375\n",
      "        vf_explained_var: 0.00015079975128173828\n",
      "        vf_loss: 21030.619140625\n",
      "    load_time_ms: 53.446\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 5987.752\n",
      "    update_time_ms: 267.766\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.425\n",
      "    ram_util_percent: 56.735000000000014\n",
      "  pid: 20702\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10238485237372365\n",
      "    mean_inference_ms: 0.9470239845965301\n",
      "    mean_processing_ms: 0.20144239334356276\n",
      "  time_since_restore: 30.810847759246826\n",
      "  time_this_iter_s: 13.643014192581177\n",
      "  time_total_s: 30.810847759246826\n",
      "  timestamp: 1584822496\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6754f238\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:28:16,210\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (21030.619140625) compared to the policy loss (-0.008683229796588421). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_675408dc:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-27\n",
      "  done: true\n",
      "  episode_len_mean: 36.22522522522522\n",
      "  episode_reward_max: 986.0\n",
      "  episode_reward_mean: 577.5135135135135\n",
      "  episode_reward_min: 346.0\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 424\n",
      "  experiment_id: 3f0d15625fc04812b1ee5b0a15e27735\n",
      "  experiment_tag: 0_lr=0.01\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8182.239\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 1.8523340225219727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.27892714738845825\n",
      "        policy_loss: 0.05817447975277901\n",
      "        total_loss: 64823.04296875\n",
      "        vf_explained_var: 3.845460838647341e-09\n",
      "        vf_loss: 64822.859375\n",
      "    load_time_ms: 50.028\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5553.745\n",
      "    update_time_ms: 194.071\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.95294117647059\n",
      "    ram_util_percent: 56.594117647058816\n",
      "  pid: 20699\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10375000717103332\n",
      "    mean_inference_ms: 0.9271708362033575\n",
      "    mean_processing_ms: 0.19471878389966107\n",
      "  time_since_restore: 42.05967903137207\n",
      "  time_this_iter_s: 11.512390375137329\n",
      "  time_total_s: 42.05967903137207\n",
      "  timestamp: 1584822507\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 675408dc\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20699)\u001b[0m 2020-03-21 15:28:27,069\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (64822.859375) compared to the policy loss (0.05817447975277901). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/8 CPUs, 0/1 GPUs, 0.0/4.35 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (1 TERMINATED, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_675408dc</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         42.0597</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 577.514</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_67546818</td><td>RUNNING   </td><td>192.168.0.11:20695</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.7899</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 271.457</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_6754f238</td><td>RUNNING   </td><td>192.168.0.11:20702</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.8108</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\"> 259.547</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Knapsack-v0_67546818:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-27\n",
      "  done: true\n",
      "  episode_len_mean: 22.710227272727273\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 308.02840909090907\n",
      "  episode_reward_min: 172.0\n",
      "  episodes_this_iter: 176\n",
      "  episodes_total: 557\n",
      "  experiment_id: c0b1e1281d2d4a4385e0b44b4c6a1487\n",
      "  experiment_tag: 1_lr=0.0001\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8355.008\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 5.16617488861084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027405565604567528\n",
      "        policy_loss: -0.04151194170117378\n",
      "        total_loss: 20707.4921875\n",
      "        vf_explained_var: 7.773599463689607e-06\n",
      "        vf_loss: 20707.517578125\n",
      "    load_time_ms: 59.654\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5470.404\n",
      "    update_time_ms: 192.403\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03125\n",
      "    ram_util_percent: 56.5875\n",
      "  pid: 20695\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.09791774160116376\n",
      "    mean_inference_ms: 0.9004408772235493\n",
      "    mean_processing_ms: 0.19293784618417423\n",
      "  time_since_restore: 42.3313627243042\n",
      "  time_this_iter_s: 11.541496515274048\n",
      "  time_total_s: 42.3313627243042\n",
      "  timestamp: 1584822507\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '67546818'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=20695)\u001b[0m 2020-03-21 15:28:27,562\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20707.517578125) compared to the policy loss (-0.04151194170117378). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n",
      "Result for PPO_Knapsack-v0_6754f238:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-21_15-28-27\n",
      "  done: true\n",
      "  episode_len_mean: 21.01578947368421\n",
      "  episode_reward_max: 397.0\n",
      "  episode_reward_mean: 260.0210526315789\n",
      "  episode_reward_min: 155.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 573\n",
      "  experiment_id: 4d8aa7eb24f74eba9f13241a555b02f6\n",
      "  experiment_tag: 2_lr=1e-06\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 8401.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 9.999999974752427e-07\n",
      "        entropy: 5.294013023376465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0012593501014634967\n",
      "        policy_loss: -0.01494651474058628\n",
      "        total_loss: 20297.87109375\n",
      "        vf_explained_var: 7.515953620895743e-05\n",
      "        vf_loss: 20297.884765625\n",
      "    load_time_ms: 42.992\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5525.726\n",
      "    update_time_ms: 179.826\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.09375\n",
      "    ram_util_percent: 56.6125\n",
      "  pid: 20702\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.10100140908133759\n",
      "    mean_inference_ms: 0.913098358867586\n",
      "    mean_processing_ms: 0.19638155531996473\n",
      "  time_since_restore: 42.53188467025757\n",
      "  time_this_iter_s: 11.721036911010742\n",
      "  time_total_s: 42.53188467025757\n",
      "  timestamp: 1584822507\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6754f238\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.35 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Knapsack-v0_675408dc</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         42.0597</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 577.514</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_67546818</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         42.3314</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 308.028</td></tr>\n",
       "<tr><td>PPO_Knapsack-v0_6754f238</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         42.5319</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\"> 260.021</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 15:28:27,995\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20702)\u001b[0m 2020-03-21 15:28:27,949\tWARNING ppo.py:129 -- The magnitude of your value function loss is extremely large (20297.884765625) compared to the policy loss (-0.01494651474058628). This can prevent the policy from learning. Consider scaling down the VF loss by reducing vf_loss_coeff, or disabling vf_share_layers.\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Knapsack-v0'\n",
    "# env = rl_utils.create_env(env_name)\n",
    "{'env_config': {'reuse_actors': True}, \n",
    " 'vf_clip_param': 10, \n",
    " 'model': {'fcnet_activation': 'elu', 'fcnet_hiddens': [128, 128, 128]}}\n",
    "# Can also register the env creator function explicitly with:\n",
    "# tune.register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "# tune.register_env(env_name, lambda env_name, config: rl_utils.create_env(env_name, config))\n",
    "# tune.register_env(env_name, lambda config: _reg_func(config))\n",
    "# tune.register_env(env_name, lambda config: KnapsackEnv(config))\n",
    "# tune.register_env(env_name, lambda config: rl_utils.create_env(config))\n",
    "# tune.register_env(env_name, lambda config: env(config))\n",
    "rl_utils.register_env(env_name)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "ModelCatalog.register_custom_model(\"my_model\", rl_utils.FCModel)\n",
    "# ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 10000,\n",
    "    },\n",
    "    config={\n",
    "        \"env\": env_name,\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"my_model\",\n",
    "        },\n",
    "        \"vf_share_layers\": True,\n",
    "        \"lr\": tune.grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n",
    "        \"num_workers\": 1,  # parallelism\n",
    "        \"env_config\": {\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dataframe().to_csv(\"x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reg_func(whatever):\n",
    "    return KnapsackEnv(whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "or_gym.envs.classic_or.knapsack.KnapsackEnv"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = _reg_func('')\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\tKnapsack-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "or_gym.envs.classic_or.knapsack.KnapsackEnv"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = rl_utils.create_env(env_name)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-03-21'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strftime(datetime.today(), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
