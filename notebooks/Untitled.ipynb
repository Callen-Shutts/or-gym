{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from collections import Iterable\n",
    "from or_gym.algos.rl_utils import *\n",
    "from ray.rllib import agents\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the NAN values!\n",
    "\n",
    "Check with random actions to see if Nan or $\\pm$ inf arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(x, msg=''):\n",
    "    if not isinstance(x, Iterable):\n",
    "        x = np.array([x])\n",
    "    if type(x) is tuple:\n",
    "        x = np.array(x).flatten()\n",
    "    if any(np.isnan(s)):\n",
    "        raise ValueError('{}\\n{}'.format(msg, x))\n",
    "    elif any(x==np.inf):\n",
    "        raise ValueError('{}\\n{}'.format(msg, x))\n",
    "    elif any(x==-np.inf):\n",
    "        raise ValueError('{}\\n{}'.format(msg, x))\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'InvManagement-v0'\n",
    "env = or_gym.make('InvManagement-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "rewards = []\n",
    "for i in range(N):\n",
    "    R = []\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while done == False:\n",
    "        check_values(s, 'Error found in state.')\n",
    "        action = env.sample_action()\n",
    "        check_values(action, 'Error found in action.')\n",
    "#         action = np.zeros(action.shape)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        R.append(r)\n",
    "        check_values(r, 'Error found in reward')\n",
    "        if done:\n",
    "            rewards.append(max(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.189072499999995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See if Nan's arise via Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 17:54:56,049\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-05-06 17:54:56,051\tINFO resource_spec.py:216 -- Starting Ray with 4.64 GiB memory available for workers and up to 2.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-06 17:54:56,634\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-06 17:54:56,651\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-05-06 17:55:15,938\tINFO trainable.py:102 -- _setup took 19.287 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-06 17:55:15,939\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "trainer = agents.a3c.A3CTrainer(env=create_env(env_name),\n",
    "    config={\n",
    "        'env_config': {\n",
    "            'mask': True\n",
    "        },\n",
    "    'model': {\n",
    "        'fcnet_activation': 'elu',\n",
    "        'fcnet_hiddens': [128, 128]\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 17:57:23,870\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode: 333\tMean Rewards: -347.0\tEpisodes/sec: 0.72\tTotal Time: 387.6s"
     ]
    }
   ],
   "source": [
    "trainer = agents.ddpg.DDPGTrainer(env=create_env(env_name),\n",
    "    config={\n",
    "        'env_config': {\n",
    "            'mask': True\n",
    "        },\n",
    "    'model': {\n",
    "        'fcnet_activation': 'elu',\n",
    "        'fcnet_hiddens': [128, 128]\n",
    "        }\n",
    "    })\n",
    "\n",
    "training = True\n",
    "n_episodes = 10000\n",
    "batch = 0\n",
    "rewards, eps, eps_total = [], [], []\n",
    "t_start = time.time()\n",
    "while training:\n",
    "    t_batch = time.time()\n",
    "    results = trainer.train()\n",
    "    rewards.append(results['episode_reward_mean'])\n",
    "    eps.append(results['episodes_this_iter'])\n",
    "    eps_total.append(results['episodes_total'])\n",
    "    batch += 1\n",
    "    t_end = time.time()\n",
    "    if sum(eps) >= n_episodes:\n",
    "        training = False\n",
    "        break\n",
    "    if batch % 10 == 0:\n",
    "        t = t_end - t_batch\n",
    "        t_tot = t_end - t_start\n",
    "        print(\"\\rEpisode: {}\\tMean Rewards: {:.1f}\\tEpisodes/sec: {:.2f}\\tTotal Time: {:.1f}s\".format(\n",
    "            eps_total[-1], rewards[-1], eps[-1]/t, t_tot), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_action(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = trainer.env_creator(config={\n",
    "        'env_config': {\n",
    "#             'env': 'InvManagement-v0',\n",
    "            'env': 'Knapsack-v0',\n",
    "            'mask': True\n",
    "        },\n",
    "    'model': {\n",
    "        'fcnet_activation': 'elu',\n",
    "        'fcnet_hiddens': [128, 128]\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains(np.array([1, 1, 90]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  90,  80])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.supply_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "box requires scalar bounds. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dd1405ab18a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupply_capacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/spaces/box.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, low, high, shape, dtype)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'box requires scalar bounds. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: box requires scalar bounds. "
     ]
    }
   ],
   "source": [
    "spaces.Box(low=np.zeros(3), high=env.supply_capacity, shape=(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Box(1,), Box(1,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaces.Tuple((\n",
    "    spaces.Box(0, env.supply_capacity[0], shape=(1,)),\n",
    "    spaces.Box(0, env.supply_capacity[1], shape=(1,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spaces.Tuple(tuple([spaces.Box(0, i, shape=(1,)) for i in env.supply_capacity]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(x.sample())) is type(np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = x.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(action) != type(np.array([])):\n",
    "    R = np.array(action).flatten().astype(int)\n",
    "\n",
    "# get inventory at hand and pipeline inventory at beginning of the period\n",
    "n = env.period\n",
    "L = env.lead_time\n",
    "I = env.I[n,:].copy() # inventory at start of period n\n",
    "T = env.T[n,:].copy() # pipeline inventory at start of period n\n",
    "m = env.num_stages # number of stages\n",
    "\n",
    "c = env.supply_capacity # capacity\n",
    "        \n",
    "# available inventory at the m+1 stage (note: last stage has unlimited supply)\n",
    "Im1 = np.append(I[1:], np.Inf) \n",
    "\n",
    "# place replenishment order\n",
    "# R = action.astype(int)\n",
    "R[R<0] = 0 # force non-negativity\n",
    "if n>=1: # add backlogged replenishment orders to current request\n",
    "    R = R + env.B[n-1,1:]\n",
    "Rcopy = R.copy() # copy original replenishment quantity\n",
    "R[R>=c] = c[R>=c] # enforce capacity constraint\n",
    "R[R>=Im1] = Im1[R>=Im1] # enforce available inventory constraint\n",
    "env.R[n,:] = R # store R[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [51],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
