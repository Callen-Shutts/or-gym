{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import or_gym\n",
    "from pyomo.environ import *\n",
    "from or_gym.algos.math_prog_utils import *\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from collections import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Knapsack-v2'\n",
    "env = gym.make(env_name)\n",
    "env.item_values = np.array([10, 5])\n",
    "env.item_probs = np.array([0.75, 0.25])\n",
    "env.item_weights = np.array([2, 1])\n",
    "env.N = 2\n",
    "env.item_numbers = np.arange(env.N)\n",
    "env.step_limit = 2\n",
    "\n",
    "# Initialize model\n",
    "m = ConcreteModel()\n",
    "\n",
    "# Sets, parameters, and variables\n",
    "m.W = env.max_weight\n",
    "m.T = env.step_limit\n",
    "\n",
    "m.i = Set(initialize=env.item_numbers)\n",
    "m.t = RangeSet(0, m.T - 1) # Time steps\n",
    "m.s = Set(initialize=env.item_numbers) # Scenarios\n",
    "\n",
    "m.w = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_weights)})\n",
    "m.v = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_values)})\n",
    "m.p = Param(m.i,\n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_probs)})\n",
    "\n",
    "m.x = Var(m.i, m.t, m.s, within=Binary)\n",
    "\n",
    "@m.Constraint()\n",
    "def weight_constraint(m):\n",
    "    return sum(m.w[i] * m.x[i, t, s] \n",
    "               for i in m.i\n",
    "               for t in m.t \n",
    "               for s in m.s) - m.W <= 0\n",
    "\n",
    "@m.Constraint(m.t, m.s)\n",
    "def assignment_constraint(m, t, s):\n",
    "    return sum(m.x[i, t, s] \n",
    "               for i in m.i\n",
    "               for s in m.s) <= 1\n",
    "\n",
    "@m.Constraint(m.i, m.t, m.s)\n",
    "def scenario_constraints(m, i, t, s):\n",
    "    if i == s:\n",
    "        return (m.x[i, t, s] <= 1)\n",
    "    else:\n",
    "        return (m.x[i, t, s] == 0)\n",
    "\n",
    "\n",
    "m.obj = Objective(expr=(\n",
    "    sum([m.v[i] * m.x[i, t, s] \n",
    "         for i in m.i\n",
    "         for t in m.t\n",
    "         for s in m.s])),\n",
    "    sense=maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file /tmp/tmph5_8jd8h.pyomo.lp\n",
      "Reading time = 0.00 seconds\n",
      "x9: 14 rows, 9 columns, 33 nonzeros\n",
      "Optimize a model with 14 rows, 9 columns and 33 nonzeros\n",
      "Variable types: 1 continuous, 8 integer (8 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 2e+00]\n",
      "  Objective range  [5e+00, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+01]\n",
      "Presolve removed 14 rows and 9 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds\n",
      "Thread count was 1 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 20 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+01, best bound 2.000000000000e+01, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "model, results = solve_math_program(m, solver='gurobi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.obj.expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0) 1.0\n",
      "(0, 1, 0) 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i, m.x[i].value) for i in m.x if m.x[i].value > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file /tmp/tmpd4exmg_i.pyomo.lp\n",
      "Reading time = 0.00 seconds\n",
      "x5: 4 rows, 5 columns, 9 nonzeros\n",
      "Optimize a model with 4 rows, 5 columns and 9 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [2e-01, 2e+00]\n",
      "  Objective range  [1e+00, 8e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 2e+01]\n",
      "Presolve removed 4 rows and 5 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.5000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds\n",
      "Optimal objective  1.500000000e+01\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "# LP Approximation\n",
    "env_name = 'Knapsack-v2'\n",
    "env = gym.make(env_name)\n",
    "env.item_values = np.array([10, 5])\n",
    "env.item_probs = np.array([0.75, 0.25])\n",
    "env.item_weights = np.array([2, 1])\n",
    "env.N = 2\n",
    "env.item_numbers = np.arange(env.N)\n",
    "env.step_limit = 2\n",
    "\n",
    "# Initialize model\n",
    "m = ConcreteModel()\n",
    "\n",
    "# Sets, parameters, and variables\n",
    "m.W = env.max_weight\n",
    "m.T = env.step_limit\n",
    "\n",
    "m.i = Set(initialize=env.item_numbers)\n",
    "m.t = RangeSet(0, m.T - 1) # Time steps\n",
    "m.s = Set(initialize=env.item_numbers) # Scenarios\n",
    "\n",
    "m.w = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_weights)})\n",
    "m.v = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_values)})\n",
    "m.p = Param(m.i,\n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_probs)})\n",
    "\n",
    "m.x = Var(m.i, m.t, within=NonNegativeReals)\n",
    "\n",
    "@m.Constraint()\n",
    "def weight_constraint(m):\n",
    "    return sum(m.w[i] * m.x[i, t] * m.p[i]\n",
    "               for i in m.i\n",
    "               for t in m.t) - m.W <= 0\n",
    "\n",
    "@m.Constraint(m.t)\n",
    "def assignment_constraint(m, t):\n",
    "    return sum(m.x[i, t]\n",
    "               for i in m.i) <= 1\n",
    "\n",
    "# @m.Constraint(m.i, m.t)\n",
    "# def scenario_constraints(m, i, t):\n",
    "#     if i == s:\n",
    "#         return (m.x[i, t, s] <= 1)\n",
    "#     else:\n",
    "#         return (m.x[i, t, s] == 0)\n",
    "\n",
    "\n",
    "m.obj = Objective(expr=(\n",
    "    sum([m.v[i] * m.x[i, t] * m.p[i]\n",
    "         for i in m.i\n",
    "         for t in m.t])),\n",
    "    sense=maximize)\n",
    "\n",
    "model, results = solve_math_program(m, 'gurobi')\n",
    "print(model.obj.expr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.x[i].value for i in m.x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Knapsack Heuristic\n",
    "\n",
    "Naive greedy algorithm, use as a placeholder. Take the item if the item fits and is greater than the average weighted value density of all items. Decrease this threshold with each sample until the sack is full or the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Knapsack-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "vw_ratio = env.item_values / env.item_weights\n",
    "T = np.mean(vw_ratio)\n",
    "item = copy.copy(env.current_item)\n",
    "done = False\n",
    "actions = []\n",
    "items_taken = []\n",
    "items_offered = []\n",
    "rewards = []\n",
    "count = 0\n",
    "while not done:\n",
    "    if env.item_weights[item] >= (env.max_weight - env.current_weight):\n",
    "        action = 0\n",
    "    elif vw_ratio[item] >= T / (1 + count):\n",
    "        action = 1\n",
    "        items_taken.append(item)\n",
    "    else:\n",
    "        action = 0\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    items_offered.append(item)\n",
    "    item = state[-1][-1]\n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okp_heuristic_decay(env, scenario=None):\n",
    "    assert env.spec.id == 'Knapsack-v2', \\\n",
    "        '{} received. Heuristic designed for Knapsack-v2.'.format(env.spec.id)\n",
    "    if scenario is not None:\n",
    "        # Ensure scenario is iterable of length step_limit\n",
    "        assert isinstance(scenario, Iterable), 'scenario not iterable.'\n",
    "        assert len(scenario) >= env.step_limit, 'scenario too short.'\n",
    "    env.reset()\n",
    "\n",
    "    vw_ratio = env.item_values / env.item_weights\n",
    "    T = np.mean(vw_ratio)\n",
    "    done = False\n",
    "    actions = []\n",
    "    items_taken = []\n",
    "    items_offered = []\n",
    "    rewards = []\n",
    "    count = 0\n",
    "    while not done:\n",
    "        if scenario is not None:\n",
    "            item = scenario[count]\n",
    "        else:\n",
    "            item = copy.copy(env.current_item)\n",
    "        if env.item_weights[item] >= (env.max_weight - env.current_weight):\n",
    "            action = 0\n",
    "        elif vw_ratio[item] >= T / (1 + count):\n",
    "            action = 1\n",
    "            items_taken.append(item)\n",
    "        else:\n",
    "            action = 0\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        items_offered.append(item)\n",
    "        count += 1\n",
    "\n",
    "    return actions, items_offered, rewards\n",
    "\n",
    "def okp_heuristic(env, scenario=None):\n",
    "    assert env.spec.id == 'Knapsack-v2', \\\n",
    "        '{} received. Heuristic designed for Knapsack-v2.'.format(env.spec.id)\n",
    "    if scenario is not None:\n",
    "        # Ensure scenario is iterable of length step_limit\n",
    "        assert isinstance(scenario, Iterable), 'scenario not iterable.'\n",
    "        assert len(scenario) >= env.step_limit, 'scenario too short.'\n",
    "    env.reset()\n",
    "\n",
    "    vw_ratio = env.item_values / env.item_weights\n",
    "    T = np.mean(vw_ratio)\n",
    "    done = False\n",
    "    actions = []\n",
    "    items_taken = []\n",
    "    items_offered = []\n",
    "    rewards = []\n",
    "    count = 0\n",
    "    while not done:\n",
    "        if scenario is not None:\n",
    "            item = scenario[count]\n",
    "        else:\n",
    "            item = copy.copy(env.current_item)\n",
    "        if env.item_weights[item] >= (env.max_weight - env.current_weight):\n",
    "            action = 0\n",
    "        elif vw_ratio[item] >= T:\n",
    "            action = 1\n",
    "            items_taken.append(item)\n",
    "        else:\n",
    "            action = 0\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        items_offered.append(item)\n",
    "        count += 1\n",
    "\n",
    "    return actions, items_offered, rewards\n",
    "\n",
    "def two_bin(env, scenario):\n",
    "    assert env.spec.id == 'Knapsack-v2', \\\n",
    "        '{} received. Heuristic designed for Knapsack-v2.'.format(env.spec.id)\n",
    "    if scenario is not None:\n",
    "        # Ensure scenario is iterable of length step_limit\n",
    "        assert isinstance(scenario, Iterable), 'scenario not iterable.'\n",
    "        assert len(scenario) >= env.step_limit, 'scenario too short.'\n",
    "    env.reset()\n",
    "    \n",
    "    done = False\n",
    "    actions = []\n",
    "    items_taken = []\n",
    "    items_offered = []\n",
    "    rewards = []\n",
    "    count = 0\n",
    "    while not done:\n",
    "        if scenario is not None:\n",
    "            item = scenario[count]\n",
    "        else:\n",
    "            item = copy.copy(env.current_item)\n",
    "            \n",
    "        r = bool(np.random.choice([0, 1]))\n",
    "        action = 0\n",
    "        if r:\n",
    "            # Greedy algorithm\n",
    "            if env.item_weights[item] <= (env.max_weight - env.current_weight):\n",
    "                action = 1\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        items_offered.append(item)\n",
    "        count += 1\n",
    "\n",
    "    return actions, items_offered, rewards\n",
    "\n",
    "def two_bin2(env, scenario=None):\n",
    "    '''TwoBins from Han 2015'''\n",
    "    assert env.spec.id == 'Knapsack-v2', \\\n",
    "        '{} received. Heuristic designed for Knapsack-v2.'.format(env.spec.id)\n",
    "    if scenario is not None:\n",
    "        # Ensure scenario is iterable of length step_limit\n",
    "        assert isinstance(scenario, Iterable), 'scenario not iterable.'\n",
    "        assert len(scenario) >= env.step_limit, 'scenario too short.'\n",
    "    env.reset()\n",
    "    \n",
    "    done = False\n",
    "    actions = []\n",
    "    items_taken = []\n",
    "    items_offered = []\n",
    "    rewards = []\n",
    "    r = bool(np.random.choice([0, 1]))\n",
    "    rejection_weight = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        if scenario is not None:\n",
    "            item = scenario[count]\n",
    "        else:\n",
    "            item = copy.copy(env.current_item)\n",
    "        action = 0\n",
    "        if r:\n",
    "            # Greedy algorithm\n",
    "            if env.item_weights[item] <= (env.max_weight - env.current_weight):\n",
    "                action = 1\n",
    "        else:\n",
    "        \trejection_weight += env.item_weights[item]\n",
    "        \tif rejection_weight > env.max_weight:\n",
    "        \t\taction = 1\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        items_offered.append(item)\n",
    "        count += 1\n",
    "\n",
    "    return actions, items_offered, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Greedy Decay Heuristic Reward\t=\t139.39\n",
      "15.48s elapsed\n",
      "Average Greedy no Decay Heuristic Reward\t=\t139.16\n",
      "15.57s elapsed\n",
      "Average TwoBins Heuristic Reward\t=\t263.80\n",
      "21.97s elapsed\n",
      "Average TwoBin 2 Heuristic Reward\t=\t269.68\n",
      "11.45s elapsed\n"
     ]
    }
   ],
   "source": [
    "# Compare heuristic approaches with and without decay\n",
    "# Keep items constant across all applications\n",
    "env = gym.make('Knapsack-v2')\n",
    "N_SCENARIOS = 10000\n",
    "item_sequence = np.random.choice(env.item_numbers, \n",
    "    size=(N_SCENARIOS, env.step_limit), p=env.item_probs)\n",
    "t0 = time.time()\n",
    "avg_dec_rewards = 0\n",
    "for n in range(N_SCENARIOS):\n",
    "    env.reset()\n",
    "    actions, items, rewards = okp_heuristic(env, item_sequence[n])\n",
    "    avg_dec_rewards += (sum(rewards) - avg_dec_rewards) / (n + 1)\n",
    "print(\"Average Greedy Decay Heuristic Reward\\t=\\t{:.2f}\".format(avg_dec_rewards))\n",
    "print(\"{:.2f}s elapsed\".format(time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "avg_heur_rewards = 0\n",
    "for n in range(N_SCENARIOS):\n",
    "    env.reset()\n",
    "    actions, items, rewards = okp_heuristic(env, item_sequence[n])\n",
    "    avg_heur_rewards += (sum(rewards) - avg_heur_rewards) / (n + 1)\n",
    "    \n",
    "print(\"Average Greedy no Decay Heuristic Reward\\t=\\t{:.2f}\".format(avg_heur_rewards))\n",
    "print(\"{:.2f}s elapsed\".format(time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "avg_twobin_rewards = 0\n",
    "for n in range(N_SCENARIOS):\n",
    "    env.reset()\n",
    "    actions, items, rewards = two_bin(env, item_sequence[n])\n",
    "    avg_twobin_rewards += (sum(rewards) - avg_twobin_rewards) / (n + 1)\n",
    "    \n",
    "print(\"Average TwoBins Heuristic Reward\\t=\\t{:.2f}\".format(avg_twobin_rewards))\n",
    "print(\"{:.2f}s elapsed\".format(time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "avg_twobin2_rewards = 0\n",
    "for n in range(N_SCENARIOS):\n",
    "    env.reset()\n",
    "    actions, items, rewards = two_bin2(env, item_sequence[n])\n",
    "    avg_twobin2_rewards += (sum(rewards) - avg_twobin2_rewards) / (n + 1)\n",
    "    \n",
    "print(\"Average TwoBin 2 Heuristic Reward\\t=\\t{:.2f}\".format(avg_twobin2_rewards))\n",
    "print(\"{:.2f}s elapsed\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
